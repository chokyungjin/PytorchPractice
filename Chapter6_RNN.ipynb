{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter6_RNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"6h5wivMenJyb","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssSDhnVunc87","colab_type":"code","colab":{}},"source":["n_hidden = 35\n","# 순환 신경망의 노드수 = 35\n","lr = 0.01\n","epochs = 1000\n","\n","string = \"hello pytorch. how long can a rnn cell remember?\"\n","chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n","\n","# 예시에 사용할 문장은 string에 사용할 문자, 기호들은 chars 에 저장\n","char_list = [i for i in chars]\n","# chars 의 요소들을 리스트로 만들어줌 \n","n_letters = len(char_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"29BYmj22n_i1","colab_type":"code","colab":{}},"source":["# 문자를 그대로 쓰지않고 one-hot 벡터로 바꿔서 연산에 쓰도록 하겠습니다.\n","\n","#Start = [0 0 0 … 1 0]\n","#a =     [1 0 0 … 0 0]\n","#b =     [0 1 0 … 0 0]\n","#c =     [0 0 1 … 0 0]\n","#...\n","#end =   [0 0 0 … 0 1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HNiiaxdro746","colab_type":"code","colab":{}},"source":["def string_to_onehot(string):\n","  # 문자를 one_hot으로 만드는 함수\n","  start = np.zeros(shape=len(char_list),dtype=int)\n","  end = np.zeros(shape=len(char_list), dtype=int)\n","  start[-2] = 1\n","  # start 맨 뒤에서 두번째는 1\n","  end[-1] = 1\n","  # end 맨 뒤에는 1\n","  for i in string:\n","    idx = char_list.index(i)\n","    zero = np.zeros(shape=n_letters,dtype=int)\n","    # zero 는 list의 길이의 0 배열 \n","    zero[idx] = 1\n","    # 해당 인덱스에 1 넣고 나머지는 다 0\n","\n","    start = np.vstack([start,zero])\n","    # start 에 [start,zero] 의 배열을 추가함 \n","  output = np.vstack([start,end])\n","  # output 에 [start,end]의 배열을 추가함\n","  return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rAJ5xpSpawQ","colab_type":"code","colab":{}},"source":["def one_hot_to_word(onehot_1):\n","  # one_hot을 다시 문자로 만드는 함수\n","  onehot = torch.Tensor.numpy(onehot_1)\n","  # 토치 텐서를 입력으로 받아서 이를 넘파이 배열로 변환\n","  # onehot의 argmax를 찾아서 다시 char_list로 반환\n","  return char_list[onehot.argmax()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_1Fb_m_plM1","colab_type":"code","colab":{}},"source":["class RNN(nn.Module):\n","  def __init__(self, input_size, hidden_size, output_size):\n","    super(RNN, self).__init__()\n","    # input_size, output_size 는 배열의 길이\n","    # hidden_size = 35\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","\n","    # 단어 하나를 입력값으로 받고 은닉층 하나를 통과시켜 결괏값을 내는 구조\n","    self.i2h = nn.Linear(input_size,hidden_size)\n","    self.h2h = nn.Linear(hidden_size,hidden_size)\n","    self.i2o = nn.Linear(hidden_size,output_size)\n","    self.act_fn = nn.Tanh()\n","    # 활성화 함수는 Tanh \n","\n","  def forward(self,input,hidden):\n","    hidden = self.act_fn(self.i2h(input)+self.h2h(hidden))\n","    output = self.i2o(hidden)\n","    return output, hidden\n","\n","  def init_hidden(self):\n","    # 이전 시간의 은닉층 연산값이 없는 초기의 은닉층 값은 0으로 초기화\n","    return torch.zeros(1,self.hidden_size)\n","\n","  \n","rnn = RNN(n_letters,n_hidden,n_letters)\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTVxCDxuqYHz","colab_type":"code","outputId":"9ac08324-1200-42e6-9aaa-3b6a31b7ed16","executionInfo":{"status":"ok","timestamp":1572964476711,"user_tz":-540,"elapsed":14684,"user":{"displayName":"조경진","photoUrl":"","userId":"06712751099335327791"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["loss_func = nn.MSELoss()\n","# 손실함수 MSE\n","optimizer = torch.optim.Adam(rnn.parameters(),lr=lr)\n","\n","one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n","# 학습하고자 했던 문장을 원-핫 벡터로 변환한 넘파이 배열을 토치 텐서형태로 형 변환\n","# 앞서 만든 함수대로 start_token + 문장 + end_token 형태의 행렬로 생성 \n","# 학습 부분.. 문장 전체를 학습하는 과정은 epochs 에 지정한 만큼 반복한다 \n","for i in range(epochs):\n","  rnn.zero_grad()\n","  total_loss = 0\n","  hidden = rnn.init_hidden()\n","\n","  for j in range(one_hot.size()[0]-1):\n","    # 원-핫 벡터는 문장에 있는 단어 순서대로 배열되어 있기 때문에 j번째 인덱스에 해당하는 값이 입력으로\n","    # 들어오면 j+1 해당하는 값이 target이 된다.\n","    input_ = one_hot[j:j+1,:]\n","    target = one_hot[j+1]\n","\n","    output,hidden = rnn.forward(input_,hidden)\n","    # forward 를 통해 input_ , hidden 을 output, hidden 으로 전달\n","    loss = loss_func(output.view(-1),target.view(-1))\n","    # 손실함수는 view 를 거쳐 이쁘게 들어감\n","    total_loss += loss\n","    input_ = output\n","\n","  total_loss.backward()\n","  optimizer.step()\n","\n","  if i % 10 ==0:\n","    print(total_loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(0.0762, grad_fn=<AddBackward0>)\n","tensor(0.2071, grad_fn=<AddBackward0>)\n","tensor(0.1342, grad_fn=<AddBackward0>)\n","tensor(0.1019, grad_fn=<AddBackward0>)\n","tensor(0.0836, grad_fn=<AddBackward0>)\n","tensor(0.0720, grad_fn=<AddBackward0>)\n","tensor(0.0632, grad_fn=<AddBackward0>)\n","tensor(0.0561, grad_fn=<AddBackward0>)\n","tensor(0.0501, grad_fn=<AddBackward0>)\n","tensor(0.0600, grad_fn=<AddBackward0>)\n","tensor(0.0444, grad_fn=<AddBackward0>)\n","tensor(0.0390, grad_fn=<AddBackward0>)\n","tensor(0.0347, grad_fn=<AddBackward0>)\n","tensor(0.0314, grad_fn=<AddBackward0>)\n","tensor(0.0286, grad_fn=<AddBackward0>)\n","tensor(0.0261, grad_fn=<AddBackward0>)\n","tensor(0.0469, grad_fn=<AddBackward0>)\n","tensor(0.0279, grad_fn=<AddBackward0>)\n","tensor(0.0233, grad_fn=<AddBackward0>)\n","tensor(0.0210, grad_fn=<AddBackward0>)\n","tensor(0.0191, grad_fn=<AddBackward0>)\n","tensor(0.0249, grad_fn=<AddBackward0>)\n","tensor(0.0203, grad_fn=<AddBackward0>)\n","tensor(0.0190, grad_fn=<AddBackward0>)\n","tensor(0.0159, grad_fn=<AddBackward0>)\n","tensor(0.0145, grad_fn=<AddBackward0>)\n","tensor(0.0250, grad_fn=<AddBackward0>)\n","tensor(0.0143, grad_fn=<AddBackward0>)\n","tensor(0.0161, grad_fn=<AddBackward0>)\n","tensor(0.0139, grad_fn=<AddBackward0>)\n","tensor(0.0115, grad_fn=<AddBackward0>)\n","tensor(0.0105, grad_fn=<AddBackward0>)\n","tensor(0.0103, grad_fn=<AddBackward0>)\n","tensor(0.0133, grad_fn=<AddBackward0>)\n","tensor(0.0097, grad_fn=<AddBackward0>)\n","tensor(0.0087, grad_fn=<AddBackward0>)\n","tensor(0.0187, grad_fn=<AddBackward0>)\n","tensor(0.0088, grad_fn=<AddBackward0>)\n","tensor(0.0080, grad_fn=<AddBackward0>)\n","tensor(0.0072, grad_fn=<AddBackward0>)\n","tensor(0.0068, grad_fn=<AddBackward0>)\n","tensor(0.0081, grad_fn=<AddBackward0>)\n","tensor(0.0087, grad_fn=<AddBackward0>)\n","tensor(0.0067, grad_fn=<AddBackward0>)\n","tensor(0.0061, grad_fn=<AddBackward0>)\n","tensor(0.0254, grad_fn=<AddBackward0>)\n","tensor(0.0074, grad_fn=<AddBackward0>)\n","tensor(0.0062, grad_fn=<AddBackward0>)\n","tensor(0.0055, grad_fn=<AddBackward0>)\n","tensor(0.0051, grad_fn=<AddBackward0>)\n","tensor(0.0047, grad_fn=<AddBackward0>)\n","tensor(0.0123, grad_fn=<AddBackward0>)\n","tensor(0.0112, grad_fn=<AddBackward0>)\n","tensor(0.0067, grad_fn=<AddBackward0>)\n","tensor(0.0052, grad_fn=<AddBackward0>)\n","tensor(0.0046, grad_fn=<AddBackward0>)\n","tensor(0.0043, grad_fn=<AddBackward0>)\n","tensor(0.0076, grad_fn=<AddBackward0>)\n","tensor(0.0113, grad_fn=<AddBackward0>)\n","tensor(0.0057, grad_fn=<AddBackward0>)\n","tensor(0.0046, grad_fn=<AddBackward0>)\n","tensor(0.0041, grad_fn=<AddBackward0>)\n","tensor(0.0037, grad_fn=<AddBackward0>)\n","tensor(0.0035, grad_fn=<AddBackward0>)\n","tensor(0.0158, grad_fn=<AddBackward0>)\n","tensor(0.0078, grad_fn=<AddBackward0>)\n","tensor(0.0051, grad_fn=<AddBackward0>)\n","tensor(0.0039, grad_fn=<AddBackward0>)\n","tensor(0.0036, grad_fn=<AddBackward0>)\n","tensor(0.0033, grad_fn=<AddBackward0>)\n","tensor(0.0032, grad_fn=<AddBackward0>)\n","tensor(0.0030, grad_fn=<AddBackward0>)\n","tensor(0.0029, grad_fn=<AddBackward0>)\n","tensor(0.0028, grad_fn=<AddBackward0>)\n","tensor(0.0027, grad_fn=<AddBackward0>)\n","tensor(0.0026, grad_fn=<AddBackward0>)\n","tensor(0.0025, grad_fn=<AddBackward0>)\n","tensor(0.0024, grad_fn=<AddBackward0>)\n","tensor(0.0068, grad_fn=<AddBackward0>)\n","tensor(0.0118, grad_fn=<AddBackward0>)\n","tensor(0.0052, grad_fn=<AddBackward0>)\n","tensor(0.0031, grad_fn=<AddBackward0>)\n","tensor(0.0027, grad_fn=<AddBackward0>)\n","tensor(0.0025, grad_fn=<AddBackward0>)\n","tensor(0.0023, grad_fn=<AddBackward0>)\n","tensor(0.0022, grad_fn=<AddBackward0>)\n","tensor(0.0021, grad_fn=<AddBackward0>)\n","tensor(0.0025, grad_fn=<AddBackward0>)\n","tensor(0.0061, grad_fn=<AddBackward0>)\n","tensor(0.0040, grad_fn=<AddBackward0>)\n","tensor(0.0026, grad_fn=<AddBackward0>)\n","tensor(0.0023, grad_fn=<AddBackward0>)\n","tensor(0.0020, grad_fn=<AddBackward0>)\n","tensor(0.0019, grad_fn=<AddBackward0>)\n","tensor(0.0018, grad_fn=<AddBackward0>)\n","tensor(0.0020, grad_fn=<AddBackward0>)\n","tensor(0.0186, grad_fn=<AddBackward0>)\n","tensor(0.0045, grad_fn=<AddBackward0>)\n","tensor(0.0026, grad_fn=<AddBackward0>)\n","tensor(0.0021, grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lOXs-25drEBD","colab_type":"code","outputId":"d21441a2-a7d5-4d52-b57c-891a5bc1f772","executionInfo":{"status":"ok","timestamp":1572964703305,"user_tz":-540,"elapsed":1222,"user":{"displayName":"조경진","photoUrl":"","userId":"06712751099335327791"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["start = torch.zeros(1,len(char_list))\n","# 1행 5열짜리의 zeros 배열을 만들어줌!\n","print(start)\n","start[:,-2] = 1\n","# 모든 행의 마지막 두번째 인덱스에 1 을 넣어줌 \n","\n","# 기울기 없이 테스트 한다 \n","with torch.no_grad():\n","  hidden = rnn.init_hidden()\n","  input_ = start\n","  output_string = \"\"\n","  for i in range(len(string)):\n","    output,hidden = rnn.forward(input_,hidden)\n","    # \n","    output_string += one_hot_to_word(output.data)\n","    input_ = output\n","  \n","print(output_string)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n","hello  rem bwem hwem b eemnwe. pano r.m hwem nbn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LJo5dA_OsIZx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}