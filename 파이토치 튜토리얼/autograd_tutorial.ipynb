{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Autograd: 자동 미분\n",
    "===================================\n",
    "\n",
    "PyTorch의 모든 신경망의 중심에는 ``autograd`` 패키지가 있습니다.\n",
    "먼저 이것을 가볍게 살펴본 뒤, 첫번째 신경망을 학습시켜보겠습니다.\n",
    "\n",
    "``autograd`` 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공합니다.\n",
    "이는 실행-기반-정의(define-by-run) 프레임워크로, 이는 코드를 어떻게 작성하여\n",
    "실행하느냐에 따라 역전파가 정의된다는 뜻이며, 역전파는 학습 과정의 매 단계마다\n",
    "달라집니다.\n",
    "\n",
    "더 간단한 용어로 몇 가지 예를 살펴보겠습니다.\n",
    "\n",
    "Tensor\n",
    "--------\n",
    "\n",
    "패키지의 중심에는 ``torch.Tensor`` 클래스가 있습니다. 만약 ``.requires_grad``\n",
    "속성을 ``True`` 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기\n",
    "시작합니다. 계산이 완료된 후 ``.backward()`` 를 호출하여 모든 변화도(gradient)를\n",
    "자동으로 계산할 수 있습니다. 이 Tensor의 변화도는 ``.grad`` 속성에 누적됩니다.\n",
    "\n",
    "Tensor가 기록을 추적하는 것을 중단하게 하려면, ``.detach()`` 를 호출하여 연산\n",
    "기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있습니다.\n",
    "\n",
    "기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해, 코드 블럭을\n",
    "``with torch.no_grad():`` 로 감쌀 수 있습니다. 이는 특히 변화도(gradient)는\n",
    "필요없지만, `requires_grad=True` 가 설정되어 학습 가능한 매개변수를 갖는 모델을\n",
    "평가(evaluate)할 때 유용합니다.\n",
    "\n",
    "Autograd 구현에서 매우 중요한 클래스가 하나 더 있는데, 이것은 바로 ``Function``\n",
    "클래스입니다.\n",
    "\n",
    "``Tensor`` 와 ``Function`` 은 서로 연결되어 있으며, 모든 연산 과정을\n",
    "부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성합니다. 각 tensor는\n",
    "``.grad_fn`` 속성을 갖고 있는데, 이는 ``Tensor`` 를 생성한 ``Function`` 을\n",
    "참조하고 있습니다. (단, 사용자가 만든 Tensor는 예외로, 이 때 ``grad_fn`` 은\n",
    "``None`` 입니다.)\n",
    "\n",
    "도함수를 계산하기 위해서는 ``Tensor`` 의 ``.backward()`` 를 호출하면\n",
    "됩니다. 만약 ``Tensor`` 가 스칼라(scalar)인 경우(예. 하나의 요소 값만 갖는 등)에는\n",
    "``backward`` 에 인자를 정해줄 필요가 없습니다. 하지만 여러 개의 요소를 갖고 있을\n",
    "때는 tensor의 모양을 ``gradient`` 의 인자로 지정할 필요가 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor를 생성하고 ``requires_grad=True`` 를 설정하여 연산을 기록합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor에 연산을 수행합니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` 는 연산의 결과로 생성된 것이므로 ``grad_fn`` 을 갖습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x12b7298d0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` 에 다른 연산을 수행합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>), tensor(27., grad_fn=<MeanBackward0>))\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.requires_grad_( ... )`` 는 기존 Tensor의 ``requires_grad`` 값을 바꿔치기\n",
    "(in-place)하여 변경합니다. 입력값이 지정되지 않으면 기본값은 ``False`` 입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x12b741490>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변화도(Gradient)\n",
    "-----------------\n",
    "이제 역전파(backprop)를 해보겠습니다.\n",
    "``out`` 은 하나의 스칼라 값만 갖고 있기 때문에, ``out.backward()`` 는\n",
    "``out.backward(torch.tensor(1.))`` 과 동일합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변화도 d(out)/dx를 출력합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``4.5`` 로 이루어진 행렬을 확인할 수 있습니다. ``out`` 을 *Tensor* “$o$”\n",
    "라고 하면, 다음과 같이 구할 수 있습니다.\n",
    "$o = \\frac{1}{4}\\sum_i z_i$ 이고,\n",
    "$z_i = 3(x_i+2)^2$ 이므로 $z_i\\bigr\\rvert_{x_i=1} = 27$ 입니다.\n",
    "따라서,\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$ 이므로,\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$ 입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수학적으로 벡터 함수 $\\vec{y}=f(\\vec{x})$ 에서 $\\vec{x}$ 에\n",
    "대한 $\\vec{y}$ 의 변화도는 야코비안 행렬(Jacobian Matrix)입니다:\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "일반적으로, ``torch.autograd`` 는 벡터-야코비안 곱을 계산하는 엔진입니다. 즉,\n",
    "어떤 벡터 $v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$\n",
    "에 대해 $v^{T}\\cdot J$ 을 연산합니다. 만약 $v$ 가 스칼라 함수\n",
    "$l=g\\left(\\vec{y}\\right)$ 의 기울기인 경우,\n",
    "$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$\n",
    "이며, 연쇄법칙(chain rule)에 따라 벡터-야코비안 곱은 $\\vec{x}$ 에 대한\n",
    "$l$ 의 기울기가 됩니다:\n",
    "\n",
    "\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial y_{m}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "(여기서 $v^{T}\\cdot J$ 은 $J^{T}\\cdot v$ 를 취했을 때의 열 벡터로\n",
    "취급할 수 있는 행 벡터를 갖습니다.)\n",
    "\n",
    "벡터-야코비안 곱의 이러한 특성은 스칼라가 아닌 출력을 갖는 모델에 외부 변화도를\n",
    "제공(feed)하는 것을 매우 편리하게 해줍니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 벡터-야코비안 곱의 예제를 살펴보도록 하겠습니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1352, -0.4725, -0.0398], requires_grad=True)\n",
      "11\n",
      "tensor([2048., 2048., 2048.], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x * 2\n",
    "count = 1\n",
    "# y.data.norm() = torch.sqrt(torch.sum(torch.pow(y, 2)))\n",
    "while y.data.norm() < 1000:\n",
    "    # print(y)\n",
    "    count += 1\n",
    "    y = y * 2\n",
    "print(count)\n",
    "print(y/x)\n",
    "# print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우 ``y`` 는 더 이상 스칼라 값이 아닙니다. ``torch.autograd`` 는\n",
    "전체 야코비안을 직접 계산할수는 없지만, 벡터-야코비안 곱은 간단히\n",
    "``backward`` 에 해당 벡터를 인자로 제공하여 얻을 수 있습니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n",
      "None\n",
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "# dydx = 1024!!!!!\n",
    "print(v.mul(1024))\n",
    "print(y.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 ``with torch.no_grad():`` 로 코드 블럭을 감싸서 autograd가\n",
    "``.requires_grad=True`` 인 Tensor들의 연산 기록을 추적하는 것을 멈출 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**더 읽을거리:**\n",
    "\n",
    "``autograd.Function`` 관련 문서는 https://pytorch.org/docs/stable/autograd.html#function\n",
    "에서 찾아볼 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "1\n",
      "(7,)\n",
      "(0, 1, 2)\n",
      "(array([2, 3, 4]), array([4, 5]))\n",
      "(array([0, 1]), array([0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0,1,2,3,4,5,6])\n",
    "print(t)\n",
    "\n",
    "print(t.ndim)\n",
    "print(t.shape)\n",
    "\n",
    "print(t[0],t[1],t[2])\n",
    "print(t[2:5],t[4:-1])\n",
    "print(t[:2],t[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [6 7 8]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[1,2,3],[4,5,6],[6,7,8],[1,2,3]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.FloatTensor([0,1,2,3,4,5,6])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n",
      "(tensor(0.), tensor([2., 3., 4.]), tensor([0., 1.]))\n"
     ]
    }
   ],
   "source": [
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())\n",
    "print(t[0],t[2:5],t[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3,3]])\n",
    "m2 = torch.FloatTensor([[2,2]])\n",
    "print(m1+m2)\n",
    "# 두 텐서간의 크기가 같지않아도 됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3]])\n",
    "m2 = torch.FloatTensor([[2,2]])\n",
    "print(m1+m2)\n",
    "# 두 텐서간의 크기가 같지않아도 됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.],\n",
      "        [6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3,3]])\n",
    "m2 = torch.FloatTensor([[2],[3]])\n",
    "print(m1+m2)\n",
    "# 두 텐서간의 크기가 같지않아도 됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([2, 2]), torch.Size([2, 1]))\n",
      "tensor([[18.],\n",
      "        [ 8.]])\n",
      "tensor([[6., 8.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[3,4],[1,2]])\n",
    "m2 = torch.FloatTensor([[2],[3]])\n",
    "print(m1.shape, m2.shape)\n",
    "print(m1.matmul(m2)) # matmul은 흔히 생각하는 행렬곱\n",
    "print(m1.mul(m2)) # mul은 같은 요소끼리만 곱함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n",
      "Can only calculate the mean of floating types. Got Long instead.\n"
     ]
    }
   ],
   "source": [
    "t= torch.FloatTensor([1,2])\n",
    "print(t.mean())\n",
    "t = torch.LongTensor([1,2])\n",
    "try:\n",
    "    print(t.mean())\n",
    "except Exception as exc:\n",
    "    print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1,2],[3,4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean())\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=1))\n",
    "print(t.mean(dim=-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1,2],[3,4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "print(t.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=0))\n",
    "# argmax를 리턴 - 큰 원소에 대한 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[[[0,1,2],\n",
    "              [0,1,2]]],\n",
    "             [[0,1,2],\n",
    "             [0,1,2]]])\n",
    "\n",
    "# ft = torch.FloatTensor(t)\n",
    "# print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0],[1],[2]])\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n",
      "tensor([0., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)\n",
    "# dimension의 element 개수가 1인 경우에 그 dimension을 없애줌\n",
    "x = ft.squeeze()\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.Tensor([0,1,2])\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "tensor([[0., 1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(0))\n",
    "print(ft.unsqueeze(1))\n",
    "print(ft.view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([1,2,3,4])\n",
    "print(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(lt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1, 0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "bt = torch.ByteTensor([True,False , True, False])\n",
    "print(bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor([[1,2],[3,4]])\n",
    "y = torch.LongTensor([[1,2],[3,4]])\n",
    "\n",
    "print(torch.cat([x,y],dim=0).shape)\n",
    "print(torch.cat([x,y],dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.],\n",
      "        [4., 4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([1,2,3,4])\n",
    "y = torch.FloatTensor([1,2,3,4])\n",
    "z = torch.FloatTensor([1,2,3,4])\n",
    "print(torch.stack([x,y,z],dim=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[0,1,2],[2,1,0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2],[3,4]])\n",
    "print(x.mul(2.))\n",
    "print(x)\n",
    "print(x.mul_(2.))\n",
    "# 원래의 x의 메모리에 x2를 해줌\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[4],[5],[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1,requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "# requires_grad = True, 학습할 것이라고 명시\n",
    "# W는 하나의 행렬, x 라는 input 벡터에 곱함\n",
    "hypo = x_train * W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypo - y_train)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = optim.SGD([W,b] , lr = 0.01)\n",
    "# W,b라는 학습 가능한 변수\n",
    "nb_epochs = 1000\n",
    "for epoch in range(1,nb_epochs +1):\n",
    "    hypo = x_train * W + b\n",
    "    cost = torch.mean((hypo - y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    # gradient 초기화\n",
    "    cost.backward()\n",
    "    # backward() 로 gradient 계산\n",
    "    optimizer.step()\n",
    "    # step()으로 계산된 gradient를 방향대로 Weight와 Bias, W와 b를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10 W:  0.000, Cost: 25.666666\n",
      "Epoch    1/10 W:  3.200, Cost: 5.186668\n",
      "Epoch    2/10 W:  1.920, Cost: 1.909867\n",
      "Epoch    3/10 W:  2.432, Cost: 1.385579\n",
      "Epoch    4/10 W:  2.227, Cost: 1.301692\n",
      "Epoch    5/10 W:  2.309, Cost: 1.288271\n",
      "Epoch    6/10 W:  2.276, Cost: 1.286123\n",
      "Epoch    7/10 W:  2.289, Cost: 1.285780\n",
      "Epoch    8/10 W:  2.284, Cost: 1.285725\n",
      "Epoch    9/10 W:  2.286, Cost: 1.285716\n",
      "Epoch   10/10 W:  2.285, Cost: 1.285715\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[4],[5],[6]])\n",
    "\n",
    "W = torch.zeros(1)\n",
    "lr = 0.1\n",
    "nb_epochs = 10\n",
    "\n",
    "for epoch in range(nb_epochs +1):\n",
    "    hypo = x_train * W\n",
    "    cost = torch.mean((hypo - y_train)**2)\n",
    "    gradient = torch.sum((W*x_train - y_train) * x_train)\n",
    "    \n",
    "    print('Epoch {:4d}/{} W: {: .3f}, Cost: {:.6f}'.format(\n",
    "    epoch,nb_epochs,W.item(),cost.item()\n",
    "    ))\n",
    "    W -= lr*gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Linear Regression\n",
    "x_train = torch.FloatTensor([[73,80,75],\n",
    "                            [93,88,93],\n",
    "                            [89,91,80],\n",
    "                            [96,98,100],\n",
    "                            [73,66,70],])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-29401.2012],\n",
      "        [-29360.0000],\n",
      "        [-29018.0000]])\n",
      "Epoch    0/20 hypo: tensor([0., 0., 0., 0., 0.]), Cost: 29661.800781\n",
      "tensor([[-16672.5527],\n",
      "        [-16646.4238],\n",
      "        [-16445.7031]])\n",
      "Epoch    1/20 hypo: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]), Cost: 9537.694336\n",
      "tensor([[-9456.2861],\n",
      "        [-9438.7012],\n",
      "        [-9318.0801]])\n",
      "Epoch    2/20 hypo: tensor([104.5421, 125.6208, 119.2478, 134.7862,  95.8280]), Cost: 3069.590088\n",
      "tensor([[-5365.1626],\n",
      "        [-5352.4219],\n",
      "        [-5277.2134]])\n",
      "Epoch    3/20 hypo: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]), Cost: 990.670898\n",
      "tensor([[-3045.7773],\n",
      "        [-3035.7820],\n",
      "        [-2986.3220]])\n",
      "Epoch    4/20 hypo: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]), Cost: 322.482056\n",
      "tensor([[-1730.8439],\n",
      "        [-1722.4053],\n",
      "        [-1687.5450]])\n",
      "Epoch    5/20 hypo: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]), Cost: 107.717064\n",
      "tensor([[-985.3658],\n",
      "        [-977.8093],\n",
      "        [-951.2285]])\n",
      "Epoch    6/20 hypo: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]), Cost: 38.687500\n",
      "tensor([[-562.7330],\n",
      "        [-555.6765],\n",
      "        [-533.7919]])\n",
      "Epoch    7/20 hypo: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]), Cost: 16.499043\n",
      "tensor([[-323.1293],\n",
      "        [-316.3561],\n",
      "        [-297.1362]])\n",
      "Epoch    8/20 hypo: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]), Cost: 9.365657\n",
      "tensor([[-187.2877],\n",
      "        [-180.6750],\n",
      "        [-162.9681]])\n",
      "Epoch    9/20 hypo: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]), Cost: 7.071114\n",
      "tensor([[-110.2751],\n",
      "        [-103.7533],\n",
      "        [ -86.9065]])\n",
      "Epoch   10/20 hypo: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]), Cost: 6.331847\n",
      "tensor([[-66.6136],\n",
      "        [-60.1431],\n",
      "        [-43.7862]])\n",
      "Epoch   11/20 hypo: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]), Cost: 6.092532\n",
      "tensor([[-41.8600],\n",
      "        [-35.4183],\n",
      "        [-19.3416]])\n",
      "Epoch   12/20 hypo: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]), Cost: 6.013817\n",
      "tensor([[-27.8249],\n",
      "        [-21.3994],\n",
      "        [ -5.4838]])\n",
      "Epoch   13/20 hypo: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]), Cost: 5.986785\n",
      "tensor([[-19.8704],\n",
      "        [-13.4540],\n",
      "        [  2.3680]])\n",
      "Epoch   14/20 hypo: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]), Cost: 5.976325\n",
      "tensor([[-15.3568],\n",
      "        [ -8.9453],\n",
      "        [  6.8213]])\n",
      "Epoch   15/20 hypo: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]), Cost: 5.971208\n",
      "tensor([[-12.7983],\n",
      "        [ -6.3894],\n",
      "        [  9.3434]])\n",
      "Epoch   16/20 hypo: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]), Cost: 5.967835\n",
      "tensor([[-11.3482],\n",
      "        [ -4.9407],\n",
      "        [ 10.7707]])\n",
      "Epoch   17/20 hypo: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]), Cost: 5.964969\n",
      "tensor([[-10.5218],\n",
      "        [ -4.1149],\n",
      "        [ 11.5820]])\n",
      "Epoch   18/20 hypo: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]), Cost: 5.962291\n",
      "tensor([[-10.0556],\n",
      "        [ -3.6489],\n",
      "        [ 12.0375]])\n",
      "Epoch   19/20 hypo: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]), Cost: 5.959664\n",
      "tensor([[-9.7922],\n",
      "        [-3.3853],\n",
      "        [12.2928]])\n",
      "Epoch   20/20 hypo: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]), Cost: 5.957089\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# Hx = w1x1 + w2x2 + w3x3 + b\n",
    "\n",
    "# model init\n",
    "W = torch.zeros((3,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b],lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    hypo = x_train.matmul(W) + b\n",
    "    cost = torch.mean((hypo- y_train) ** 2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    print(W.grad)\n",
    "    # print(cost.backward())\n",
    "    optimizer.step()\n",
    "    print('Epoch {:4d}/{} hypo: {}, Cost: {:.6f}'.format(\n",
    "    epoch, nb_epochs , hypo.squeeze().detach(), cost.item()\n",
    "    ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch = 전체 데이터를 균일하게 나눠서 학습하자\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73,80,75],\n",
    "                            [93,88,93],\n",
    "                            [89,91,80],\n",
    "                            [96,98,100],\n",
    "                            [73,66,70]]\n",
    "        \n",
    "        self.y_data = [[152],[185],[180],[196],[142]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    # 데이터 셋의 총 데이터수\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x,y\n",
    "    # 어떠한 인덱스 idx를 받았을 때, 그에 상응하는 입출력 데이터 반환\n",
    "    \n",
    "\n",
    "dataset = CustomDataset()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch: 1/3, Cost: 38978.574219\n",
      "Epoch    0/20 Batch: 2/3, Cost: 48398.703125\n",
      "Epoch    0/20 Batch: 3/3, Cost: 27590.240234\n",
      "Epoch    1/20 Batch: 1/3, Cost: 45641.023438\n",
      "Epoch    1/20 Batch: 2/3, Cost: 39788.000000\n",
      "Epoch    1/20 Batch: 3/3, Cost: 31486.746094\n",
      "Epoch    2/20 Batch: 1/3, Cost: 36200.945312\n",
      "Epoch    2/20 Batch: 2/3, Cost: 49228.082031\n",
      "Epoch    2/20 Batch: 3/3, Cost: 31486.746094\n",
      "Epoch    3/20 Batch: 1/3, Cost: 48398.703125\n",
      "Epoch    3/20 Batch: 2/3, Cost: 37030.320312\n",
      "Epoch    3/20 Batch: 3/3, Cost: 31486.746094\n",
      "Epoch    4/20 Batch: 1/3, Cost: 38978.574219\n",
      "Epoch    4/20 Batch: 2/3, Cost: 39788.000000\n",
      "Epoch    4/20 Batch: 3/3, Cost: 44811.648438\n",
      "Epoch    5/20 Batch: 1/3, Cost: 41736.253906\n",
      "Epoch    5/20 Batch: 2/3, Cost: 45641.023438\n",
      "Epoch    5/20 Batch: 3/3, Cost: 27590.240234\n",
      "Epoch    6/20 Batch: 1/3, Cost: 45641.023438\n",
      "Epoch    6/20 Batch: 2/3, Cost: 29538.492188\n",
      "Epoch    6/20 Batch: 3/3, Cost: 51985.761719\n",
      "Epoch    7/20 Batch: 1/3, Cost: 37030.320312\n",
      "Epoch    7/20 Batch: 2/3, Cost: 48398.703125\n",
      "Epoch    7/20 Batch: 3/3, Cost: 31486.746094\n",
      "Epoch    8/20 Batch: 1/3, Cost: 39788.000000\n",
      "Epoch    8/20 Batch: 2/3, Cost: 45641.023438\n",
      "Epoch    8/20 Batch: 3/3, Cost: 31486.746094\n",
      "Epoch    9/20 Batch: 1/3, Cost: 37030.320312\n",
      "Epoch    9/20 Batch: 2/3, Cost: 38149.195312\n",
      "Epoch    9/20 Batch: 3/3, Cost: 51985.761719\n",
      "Epoch   10/20 Batch: 1/3, Cost: 49228.082031\n",
      "Epoch   10/20 Batch: 2/3, Cost: 38149.195312\n",
      "Epoch   10/20 Batch: 3/3, Cost: 27590.240234\n",
      "Epoch   11/20 Batch: 1/3, Cost: 48398.703125\n",
      "Epoch   11/20 Batch: 2/3, Cost: 37030.320312\n",
      "Epoch   11/20 Batch: 3/3, Cost: 31486.746094\n",
      "Epoch   12/20 Batch: 1/3, Cost: 38149.195312\n",
      "Epoch   12/20 Batch: 2/3, Cost: 37030.320312\n",
      "Epoch   12/20 Batch: 3/3, Cost: 51985.761719\n",
      "Epoch   13/20 Batch: 1/3, Cost: 41736.253906\n",
      "Epoch   13/20 Batch: 2/3, Cost: 37030.320312\n",
      "Epoch   13/20 Batch: 3/3, Cost: 44811.648438\n",
      "Epoch   14/20 Batch: 1/3, Cost: 49228.082031\n",
      "Epoch   14/20 Batch: 2/3, Cost: 36200.945312\n",
      "Epoch   14/20 Batch: 3/3, Cost: 31486.746094\n",
      "Epoch   15/20 Batch: 1/3, Cost: 29538.492188\n",
      "Epoch   15/20 Batch: 2/3, Cost: 49228.082031\n",
      "Epoch   15/20 Batch: 3/3, Cost: 44811.648438\n",
      "Epoch   16/20 Batch: 1/3, Cost: 29538.492188\n",
      "Epoch   16/20 Batch: 2/3, Cost: 49228.082031\n",
      "Epoch   16/20 Batch: 3/3, Cost: 44811.648438\n",
      "Epoch   17/20 Batch: 1/3, Cost: 39788.000000\n",
      "Epoch   17/20 Batch: 2/3, Cost: 38978.574219\n",
      "Epoch   17/20 Batch: 3/3, Cost: 44811.648438\n",
      "Epoch   18/20 Batch: 1/3, Cost: 49228.082031\n",
      "Epoch   18/20 Batch: 2/3, Cost: 29538.492188\n",
      "Epoch   18/20 Batch: 3/3, Cost: 44811.648438\n",
      "Epoch   19/20 Batch: 1/3, Cost: 41736.253906\n",
      "Epoch   19/20 Batch: 2/3, Cost: 36200.945312\n",
      "Epoch   19/20 Batch: 3/3, Cost: 46470.402344\n",
      "Epoch   20/20 Batch: 1/3, Cost: 45641.023438\n",
      "Epoch   20/20 Batch: 2/3, Cost: 29538.492188\n",
      "Epoch   20/20 Batch: 3/3, Cost: 51985.761719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultivariateLinearRegressionModel,self).__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "model = MultivariateLinearRegressionModel()\n",
    "# linear model 정의\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    # minibatch 의 크기 통상적으로 2의 제곱수로 설정\n",
    "    shuffle=True,\n",
    "    # Epoch 마다 데이터셋을 섞어서, 데이터가 학습되는 순서를 바꾼다.\n",
    "    )\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    for batch_idx , samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Batch: {}/{}, Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs , batch_idx+1, len(dataloader),cost.item()\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10826d750>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2],]\n",
    "y_data = [[0],[0],[0],[1],[1],[1],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e^1 equals: ', tensor([2.7183]))\n"
     ]
    }
   ],
   "source": [
    "print('e^1 equals: ', torch.exp(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypo)\n",
    "# 기본 값이 0.5인 이유 = 1 / 1+ e^0\n",
    "print(hypo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1/(1 + e^(-1)) equals: ', tensor([0.7311]))\n"
     ]
    }
   ],
   "source": [
    "print('1/(1 + e^(-1)) equals: ', torch.sigmoid(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 가 Binary Cross Entropy이다.\n",
    "# 이 loss는 Pytorch에서 제공해줌\n",
    "# losses = -((y_train) * torch.log(hypo) + \\ \n",
    "#            (1-y_train) * torch.log(1-hypo))\n",
    "\n",
    "losses = F.binary_cross_entropy(hypo,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031673\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((2,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr=1)\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    #hypo = 1 / (1 + torch.exp(-(x_train.matmul(W)+b)))\n",
    "    hypo = torch.sigmoid(x_train.matmul(W)+b)\n",
    "    cost = F.binary_cross_entropy(hypo,y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    # 연산에 사용되었던 gradient\n",
    "    optimizer.step()\n",
    "    # step을 수행하면 cost 값을 최소화 하는 방향으로 업데이트함\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs ,cost.item()\n",
    "        )) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypo >= torch.FloatTensor([0.5])\n",
    "print(prediction[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier,self).__init__()\n",
    "        self.linear = nn.Linear(8,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2],]\n",
    "y_data = [[0],[0],[0],[1],[1],[1],]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "model = BinaryClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [6 x 2], m2: [8 x 1] at ../aten/src/TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-12a18d46cd5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnb_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chokyungjin/.local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-3abc558362af>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Chokyungjin/.local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chokyungjin/.local/lib/python2.7/site-packages/torch/nn/modules/linear.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chokyungjin/.local/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [6 x 2], m2: [8 x 1] at ../aten/src/TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypo = model(x_train)\n",
    "    \n",
    "    cost = F.binary_cross_entropy(hypo, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 ==0:\n",
    "        prediction = hypo >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
    "        epoch, nb_epochs ,cost.item(), accuracy * 100,\n",
    "        )) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
